# Explainable Pruning: Efficient and Explainable Neural Network Pruning Leveraging Neuron Death
This is the official implementation for Explainable Pruning: Efficient and Explainable Neural Network Pruning Leveraging Neuron Death.

# Overview
We challenge the conventional perspective on "dying neurons" in deep neural network training. Traditionally, this phenomenon has been perceived as undesirable due to its association with optimization difficulties and a decrease in the network's adaptability during continuous learning. However, we propose a novel perspective by examining the relationship between dying neurons and network sparsity, particularly in the context of pruning.Through systematic exploration of various hyperparameter configurations, we uncover the potential of dying neurons to facilitate structured pruning algorithms effectively. Our approach, termed "Explainable pruning," offers a means to regulate the occurrence of dying neurons, dynamically sparsing the neural network during training. Notably, our method is straightforward and widely applicable, surpassing existing structured pruning techniques while achieving results comparable to popular unstructured pruning methods.These findings mark a significant advancement, as they suggest that dying neurons can serve as an efficient model for compressing and optimizing network resources. This insight opens up new avenues for enhancing model efficiency and performance in deep learning applications.

# Results
![SGD](https://github.com/wangbst/ExplainableP/assets/97005040/0379141d-6a46-4c8e-b060-8e11e0a53e8a)
